# app/graph/nodes.py
import httpx
from typing import List
from langchain_core.embeddings import Embeddings
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_core.prompts import ChatPromptTemplate
from app.core. config import settings
from app.core.database import async_session_maker
from app.models.knowledge import KnowledgeChunk
from app.models.order import Order
from app.graph.state import AgentState
from sqlmodel import select
from pydantic import SecretStr

# ç›¸ä¼¼åº¦é˜ˆå€¼ï¼šåªæœ‰è·ç¦» < 0.5 æ‰è®¤ä¸ºç›¸å…³
SIMILARITY_THRESHOLD = 0.5

# ==========================================
# è‡ªå®šä¹‰é€šä¹‰åƒé—® Embedding é€‚é…å™¨
# ==========================================
class QwenEmbeddings(Embeddings):
    """é€šä¹‰åƒé—® Embedding API é€‚é…å™¨"""
    
    def __init__(self, base_url: str, api_key: str, model: str, dimensions: int):
        self.base_url = base_url.rstrip('/')
        self.api_key = api_key
        self.model = model
        self.dimensions = dimensions
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """åŒæ­¥æ–¹æ³•ï¼ˆä¸å®ç°ï¼‰"""
        raise NotImplementedError("è¯·ä½¿ç”¨å¼‚æ­¥æ–¹æ³• aembed_documents")
    
    def embed_query(self, text: str) -> List[float]:
        """åŒæ­¥æ–¹æ³•ï¼ˆä¸å®ç°ï¼‰"""
        raise NotImplementedError("è¯·ä½¿ç”¨å¼‚æ­¥æ–¹æ³• aembed_query")
    
    async def aembed_documents(self, texts: List[str]) -> List[List[float]]:
        """æ‰¹é‡ç”Ÿæˆ Embedding"""
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{self.base_url}/embeddings",
                headers={
                    "Authorization": f"Bearer {self. api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model":  self.model,
                    "input": texts,  # é€šä¹‰åƒé—®ä½¿ç”¨ input å‚æ•°
                    "dimensions": self.dimensions
                },
                timeout=30.0
            )
            response.raise_for_status()
            data = response.json()
            # é€šä¹‰åƒé—®è¿”å›æ ¼å¼:  {"data": [{"embedding": [... ], "index": 0}]}
            return [item["embedding"] for item in data["data"]]
    
    async def aembed_query(self, text:  str) -> List[float]:
        """å•æ¡æ–‡æœ¬ç”Ÿæˆ Embedding"""
        results = await self.aembed_documents([text])
        return results[0]


# ==========================================
# å…¨å±€ç»„ä»¶åˆå§‹åŒ–
# ==========================================

# 1. Embedding æ¨¡å‹ï¼ˆä½¿ç”¨è‡ªå®šä¹‰é€‚é…å™¨ï¼‰
embedding_model = QwenEmbeddings(
    base_url=settings. OPENAI_BASE_URL,
    api_key=settings. OPENAI_API_KEY,
    model=settings.EMBEDDING_MODEL,
    dimensions=settings.EMBEDDING_DIM
)

# 2. LLM æ¨¡å‹ (ç”¨äºç”Ÿæˆå›ç­”)
llm = ChatOpenAI(
    base_url=settings. OPENAI_BASE_URL,
    api_key=SecretStr(settings.OPENAI_API_KEY),
    model=settings.LLM_MODEL,
    temperature=0 
)

# 3. Prompt æ¨¡æ¿
PROMPT_TEMPLATE = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç”µå•†æ”¿ç­–å’¨è¯¢ä¸“å®¶ã€‚è¯·åŸºäºä»¥ä¸‹æ£€ç´¢åˆ°çš„ context å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

è§„åˆ™ï¼š
1. åªèƒ½ä¾æ® context ä¸­çš„ä¿¡æ¯å›ç­”ã€‚
2. å¦‚æœ context ä¸ºç©ºæˆ–æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·ç›´æ¥å›ç­”"æŠ±æ­‰ï¼Œæš‚æœªæŸ¥è¯¢åˆ°ç›¸å…³è§„å®š"ï¼Œä¸¥ç¦ç¼–é€ ã€‚
3. è¯­æ°”ä¸“ä¸šã€å®¢æ°”ã€‚

Context: 
{context}

User Question: 
{question}
"""

prompt = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)

# ==========================================
# èŠ‚ç‚¹å‡½æ•°å®šä¹‰
# ==========================================

async def retrieve(state: AgentState) -> dict:
    """
    æ£€ç´¢èŠ‚ç‚¹ï¼šå¸¦é˜ˆå€¼è¿‡æ»¤çš„ç¡¬é€»è¾‘
    """
    question = state["question"]
    print(f"ğŸ” [Retrieve] æ­£åœ¨æ£€ç´¢: {question}")

    # ç”ŸæˆæŸ¥è¯¢å‘é‡
    query_vector = await embedding_model. aembed_query(question)

    async with async_session_maker() as session:
        # æŸ¥è¯¢æœ€ç›¸ä¼¼çš„ chunk
        distance_col = KnowledgeChunk.embedding. cosine_distance(query_vector).label("distance") # type: ignore
        
        stmt = (
            select(KnowledgeChunk, distance_col)
            .where(KnowledgeChunk.is_active) # type: ignore
            .order_by(distance_col)
            .limit(5)
        )
        result = await session.exec(stmt)
        results = result.all() 

    # ç¡¬é€»è¾‘è¿‡æ»¤
    valid_chunks = []
    for chunk, distance in results:
        print(f"   - å†…å®¹ç‰‡æ®µ: {chunk.content[: 10]}...  | è·ç¦»åˆ†:  {distance:.4f}")
        
        if distance < SIMILARITY_THRESHOLD: 
            valid_chunks.append(chunk. content)
        else:
            print(f"   âŒ è·ç¦»è¿‡å¤§ï¼Œå·²ä¸¢å¼ƒ")

    print(f"ğŸ“„ [Retrieve] æœ€ç»ˆæœ‰æ•ˆè®°å½•: {len(valid_chunks)} æ¡")
    return {"context": valid_chunks}


# Generate èŠ‚ç‚¹çš„ System Prompt
GENERATE_SYSTEM_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªç”µå•†å®¢æœåŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„ [å‚è€ƒä¿¡æ¯] å‹å¥½åœ°å›ç­”ç”¨æˆ·ã€‚

è§„åˆ™ï¼š
1. å¦‚æœæ˜¯è®¢å•ä¿¡æ¯ï¼Œè¯·æ¸…æ™°åˆ—å‡ºè®¢å•å·ã€çŠ¶æ€ã€æ€»é¢å’Œé…é€åœ°å€ã€‚
2. å¦‚æœæ˜¯æ”¿ç­–ä¿¡æ¯ï¼Œè¯·å¼•ç”¨ç›¸å…³æ¡æ¬¾ã€‚
3. å¦‚æœå‚è€ƒä¿¡æ¯ä¸ºç©ºï¼Œè¯·ç¤¼è²Œåœ°å‘ŠçŸ¥æ— æ³•æŸ¥åˆ°ï¼Œå¹¶å¼•å¯¼ç”¨æˆ·æä¾›æ›´å¤šç»†èŠ‚ï¼ˆå¦‚å•å·ï¼‰ã€‚
4. ä¸¥ç¦ç¼–é€ æ•°æ®åº“ä¸­ä¸å­˜åœ¨çš„è®¢å•çŠ¶æ€ã€‚
"""

async def generate(state: AgentState) -> dict:
    print("ğŸ¤– [Generate] æ­£åœ¨ç”Ÿæˆç»¼åˆå›å¤...")
    
    # 1. ç»„è£…å‚è€ƒä¿¡æ¯
    context_parts = []
    
    # åŠ å…¥æ”¿ç­–èƒŒæ™¯
    if state. get("context"):
        context_parts.append("ã€ç›¸å…³æ”¿ç­–ã€‘:\n" + "\n".join(state["context"]))
    
    # åŠ å…¥è®¢å•èƒŒæ™¯
    if state.get("order_data"):
        order_raw = state["order_data"]
        if hasattr(order_raw, "model_dump"):
            order = order_raw.model_dump()
        else:
            order = order_raw or {}

        def safe_get(d, *keys, default=None):
            if not isinstance(d, dict):
                return default
            for k in keys:
                if k in d and d[k] is not None: 
                    return d[k]
            return default

        order_sn = safe_get(order, "order_sn", "sn", default="æœªçŸ¥")
        status = safe_get(order, "status", default="æœªçŸ¥")
        amount = safe_get(order, "total_amount", "amount", default=0)
        tracking = safe_get(order, "tracking_number", "tracking", "shipping_address", default=None)
        items = safe_get(order, "items", default=[])

        order_str = (
            f"ã€è®¢å•è¯¦æƒ…ã€‘:\n"
            f"- è®¢å•å·: {order_sn}\n"
            f"- å½“å‰çŠ¶æ€: {status}\n"
            f"- è®¢å•é‡‘é¢: {amount} å…ƒ\n"
            f"- æ”¶è´§åœ°å€: {tracking or 'æš‚æ— '}\n"
            f"- å•†å“æ˜ç»†:  {items}"
        )
        context_parts.append(order_str)

    context_info = "\n\n".join(context_parts) if context_parts else "æš‚æ— ç›¸å…³å‚è€ƒä¿¡æ¯ã€‚"

    # 2. æ„å»ºç”¨æˆ·æ¶ˆæ¯
    user_content = f"""[å‚è€ƒä¿¡æ¯]ï¼š
{context_info}

[ç”¨æˆ·é—®é¢˜]ï¼š
{state['question']}"""

    # 3. è°ƒç”¨ LLM
    messages = [
        SystemMessage(content=GENERATE_SYSTEM_PROMPT),
        HumanMessage(content=user_content)
    ]
    
    response = await llm.ainvoke(messages)
    
    return {"answer": response.content}


# æ„å›¾è¯†åˆ«çš„ System Prompt
INTENT_PROMPT = """ä½ æ˜¯ä¸€ä¸ªç”µå•†å®¢æœåˆ†ç±»å™¨ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç”¨æˆ·çš„è¾“å…¥ï¼Œå°†å…¶å½’ç±»ä¸ºä»¥ä¸‹ä¸‰ç§æ„å›¾ä¹‹ä¸€ï¼š
- "ORDER":  ç”¨æˆ·è¯¢é—®å…³äºä»–ä»¬è‡ªå·±çš„è®¢å•çŠ¶æ€ã€ç‰©æµã€è¯¦æƒ…ç­‰ã€‚
- "POLICY": ç”¨æˆ·è¯¢é—®å…³äºå¹³å°é€šç”¨çš„é€€æ¢è´§ã€è¿è´¹ã€æ—¶æ•ˆç­‰æ”¿ç­–ä¿¡æ¯ã€‚
- "OTHER": ç”¨æˆ·è¿›è¡Œé—²èŠã€æ‰“æ‹›å‘¼æˆ–æå‡ºä¸ä¸Šè¿°æ— å…³çš„é—®é¢˜ã€‚

åªè¿”å›åˆ†ç±»æ ‡ç­¾ï¼ˆORDER/POLICY/OTHERï¼‰ï¼Œä¸è¦è¿”å›ä»»ä½•å…¶ä»–æ–‡å­—ã€‚"""

async def intent_router(state: AgentState):
    """
    æ„å›¾è¯†åˆ«èŠ‚ç‚¹ï¼šåˆ¤æ–­ç”¨æˆ·æƒ³å¹²ä»€ä¹ˆ
    """
    print(f"ğŸ§  [Router] æ­£åœ¨åˆ†ææ„å›¾:  {state['question']}")
    
    response = await llm.ainvoke([
        SystemMessage(content=INTENT_PROMPT),
        HumanMessage(content=state["question"])
    ])
    
    intent = response.content.strip().upper()
    # å®¹é”™å¤„ç†
    if intent not in ["ORDER", "POLICY", "OTHER"]:
        intent = "OTHER"
        
    print(f"ğŸ¯ [Router] è¯†åˆ«ç»“æœ: {intent}")
    return {"intent": intent}

async def query_order(state: AgentState):
    """
    è®¢å•æŸ¥è¯¢èŠ‚ç‚¹ï¼šä»æ•°æ®åº“æŸ¥æ•°æ®
    """
    question = state["question"]
    user_id = state["user_id"]
    
    import re
    order_sn_match = re.search(r'SN\d+', question. upper())
    
    # æ„é€ æŸ¥è¯¢
    if not order_sn_match: 
        print("ğŸ” [QueryOrder] è·å–ç”¨æˆ·æœ€è¿‘è®¢å•")
        stmt = (
            select(Order)
            .where(Order.user_id == user_id)
            .order_by(Order.created_at.desc())
            .limit(1)
        )
    else:
        order_sn = order_sn_match.group()
        print(f"ğŸ” [QueryOrder] æŸ¥è¯¢è®¢å•å·: {order_sn}")
        stmt = select(Order).where(
            Order.order_sn == order_sn,
            Order.user_id == user_id 
        )

    async with async_session_maker() as session:
        result = await session.exec(stmt)
        order = result.first()

    if not order:
        return {
            "order_data": None, 
            "context": ["ç”¨æˆ·è¯¢é—®äº†è®¢å•ï¼Œä½†æ•°æ®åº“ä¸­æœªæŸ¥åˆ°ç›¸å…³è®°å½•ã€‚"]
        }
    
    # ç»„è£…è®¢å•ä¿¡æ¯
    items_str = ", ".join([f"{i['name']}(x{i['qty']})" for i in order.items])
    order_context = (
        f"è®¢å•å·: {order.order_sn}\n"
        f"çŠ¶æ€: {order.status}\n"
        f"å•†å“:  {items_str}\n"
        f"é‡‘é¢: {order.total_amount}å…ƒ\n"
        f"ç‰©æµå•å·: {order.tracking_number or 'æš‚æ— '}"
    )
    
    return {
        "order_data":  order. model_dump(), 
        "context": [order_context]
    }