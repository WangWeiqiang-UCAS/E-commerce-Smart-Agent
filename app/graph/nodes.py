# app/graph/nodes.py
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_core.prompts import ChatPromptTemplate
from sqlalchemy import select, text
from pydantic import SecretStr
from app.core.config import settings
from app.core.database import async_session_maker
from app.models.knowledge import KnowledgeChunk
from app.graph.state import AgentState
from sqlmodel import select
from app.models.order import Order

SIMILARITY_THRESHOLD = 0.5  # ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œä½äºæ­¤å€¼åˆ™è®¤ä¸ºä¸ç›¸å…³

# ==========================================
# ç»„ä»¶åˆå§‹åŒ– (å•ä¾‹æ¨¡å¼)
# ==========================================

# 1. Embedding æ¨¡å‹ (ç”¨äºæŠŠç”¨æˆ·é—®é¢˜è½¬æˆå‘é‡)
embedding_model = OpenAIEmbeddings(
    base_url=settings.OPENAI_BASE_URL,
    api_key=SecretStr(settings.OPENAI_API_KEY),
    model=settings.EMBEDDING_MODEL,
    check_embedding_ctx_length=False
)

# 2. LLM æ¨¡å‹ (ç”¨äºç”Ÿæˆå›ç­”)
# temperature=0: æ”¿ç­–å’¨è¯¢éœ€è¦ä¸¥è°¨ï¼Œä¸è¦å‘æ•£
llm = ChatOpenAI(
    base_url=settings.OPENAI_BASE_URL,
    api_key=SecretStr(settings.OPENAI_API_KEY),
    model=settings.LLM_MODEL,
    temperature=0 
)

# 3. Prompt æ¨¡æ¿
# æ ¸å¿ƒæŒ‡ä»¤ï¼šä¸¥ç¦èƒ¡ç¼–ä¹±é€  (Hallucination Guardrails)
PROMPT_TEMPLATE = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç”µå•†æ”¿ç­–å’¨è¯¢ä¸“å®¶ã€‚è¯·åŸºäºä»¥ä¸‹æ£€ç´¢åˆ°çš„ context å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

è§„åˆ™ï¼š
1. åªèƒ½ä¾æ® context ä¸­çš„ä¿¡æ¯å›ç­”ã€‚
2. å¦‚æœ context ä¸ºç©ºæˆ–æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·ç›´æ¥å›ç­”â€œæŠ±æ­‰ï¼Œæš‚æœªæŸ¥è¯¢åˆ°ç›¸å…³è§„å®šâ€ï¼Œä¸¥ç¦ç¼–é€ ã€‚
3. è¯­æ°”ä¸“ä¸šã€å®¢æ°”ã€‚

Context:
{context}

User Question:
{question}
"""

prompt = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)

# ==========================================
# èŠ‚ç‚¹å‡½æ•°å®šä¹‰
# ==========================================

async def retrieve(state: AgentState) -> dict:
    """
    æ£€ç´¢èŠ‚ç‚¹ï¼šå¸¦é˜ˆå€¼è¿‡æ»¤çš„ç¡¬é€»è¾‘
    """
    question = state["question"]
    print(f"ğŸ” [Retrieve] æ­£åœ¨æ£€ç´¢: {question}")

    query_vector = await embedding_model.aembed_query(question)

    async with async_session_maker() as session:
        # æ ¸å¿ƒä¿®æ”¹ï¼šæˆ‘ä»¬åœ¨ select æ—¶ï¼ŒæŠŠâ€œè·ç¦»æ•°å€¼â€ä¹ŸæŸ¥å‡ºæ¥
        # logic: æŸ¥å‡ºå¯¹è±¡ + è·ç¦»
        distance_col = KnowledgeChunk.embedding.cosine_distance(query_vector).label("distance") # type: ignore
        
        stmt = (
            select(KnowledgeChunk, distance_col) # ğŸ‘ˆ å¤šæŸ¥ä¸€åˆ—
            .where(KnowledgeChunk.is_active) # type: ignore

            .order_by(distance_col)
            .limit(5) # å…ˆå¤šæŸ¥å‡ æ¡ï¼Œæ–¹ä¾¿åé¢è¿‡æ»¤
        )
        result = await session.exec(stmt)
        # ç»“æœæ˜¯ [(Chunkå¯¹è±¡, 0.1), (Chunkå¯¹è±¡, 0.2)...] çš„å…ƒç»„åˆ—è¡¨
        results = result.all() 

    # --- ç¡¬é€»è¾‘è¿‡æ»¤ (Hard Filter) ---
    valid_chunks = []
    for chunk, distance in results:
        # æ‰“å°è·ç¦»æ–¹ä¾¿è°ƒè¯•ï¼ˆç”Ÿäº§ç¯å¢ƒå¯å»æ‰ï¼‰
        # distance è¶Šå°è¶Šå¥½ã€‚å¦‚æœ distance > 0.5ï¼Œè¯´æ˜ç›¸å…³æ€§å¾ˆå·®
        print(f"   - å†…å®¹ç‰‡æ®µ: {chunk.content[:10]}... | è·ç¦»åˆ†: {distance:.4f}")
        
        if distance < SIMILARITY_THRESHOLD:
            valid_chunks.append(chunk.content)
        else:
            print(f"   âŒ è·ç¦»è¿‡å¤§ï¼Œå·²ä¸¢å¼ƒ")

    # 4. æ›´æ–° State
    # å¦‚æœ valid_chunks ä¸ºç©ºï¼Œcontext å°±æ˜¯ç©ºåˆ—è¡¨ []
    # æ­¤æ—¶ Prompt é‡Œçš„ {context} å°±ä¼šæ˜¯ç©ºçš„ï¼ŒLLM å°±ä¼šæ ¹æ®è§„åˆ™ 2 å›ç­”â€œæ²¡æ‰¾åˆ°â€
    print(f"ğŸ“„ [Retrieve] æœ€ç»ˆæœ‰æ•ˆè®°å½•: {len(valid_chunks)} æ¡")
    return {"context": valid_chunks}


GENERATE_PROMPT_TEMPLATE = """
ä½ æ˜¯ä¸€ä¸ªç”µå•†å®¢æœåŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„ [å‚è€ƒä¿¡æ¯] å‹å¥½åœ°å›ç­”ç”¨æˆ·ã€‚

[å‚è€ƒä¿¡æ¯]ï¼š
{context_info}

[ç”¨æˆ·é—®é¢˜]ï¼š
{question}

è§„åˆ™ï¼š
1. å¦‚æœæ˜¯è®¢å•ä¿¡æ¯ï¼Œè¯·æ¸…æ™°åˆ—å‡ºè®¢å•å·ã€çŠ¶æ€ã€æ€»é¢å’Œé…é€åœ°å€ã€‚
2. å¦‚æœæ˜¯æ”¿ç­–ä¿¡æ¯ï¼Œè¯·å¼•ç”¨ç›¸å…³æ¡æ¬¾ã€‚
3. å¦‚æœå‚è€ƒä¿¡æ¯ä¸ºç©ºï¼Œè¯·ç¤¼è²Œåœ°å‘ŠçŸ¥æ— æ³•æŸ¥åˆ°ï¼Œå¹¶å¼•å¯¼ç”¨æˆ·æä¾›æ›´å¤šç»†èŠ‚ï¼ˆå¦‚å•å·ï¼‰ã€‚
4. ä¸¥ç¦ç¼–é€ æ•°æ®åº“ä¸­ä¸å­˜åœ¨çš„è®¢å•çŠ¶æ€ã€‚
"""

async def generate(state: AgentState) -> dict:
    print("ğŸ¤– [Generate] æ­£åœ¨ç”Ÿæˆç»¼åˆå›å¤...")
    
    # 1. ç»„è£…å‚è€ƒä¿¡æ¯
    context_parts = []
    
    # åŠ å…¥æ”¿ç­–èƒŒæ™¯
    if state.get("context"):
        context_parts.append("ã€ç›¸å…³æ”¿ç­–ã€‘:\n" + "\n".join(state["context"]))
    
    # åŠ å…¥è®¢å•èƒŒæ™¯ï¼ˆæ›´ç¨³å¥çš„è®¿é—®ï¼‰
    if state.get("order_data"):
        order_raw = state["order_data"]
        # å¦‚æœä¼ å…¥çš„æ˜¯ SQLModel å®ä¾‹ï¼Œå°è¯•è½¬ä¸º dict
        if hasattr(order_raw, "model_dump"):
            order = order_raw.model_dump()
        else:
            order = order_raw or {}

        # å®‰å…¨å–å­—æ®µï¼ˆå…¼å®¹å¤šç§å¯èƒ½çš„ key åï¼‰
        def safe_get(d, *keys, default=None):
            if not isinstance(d, dict):
                return default
            for k in keys:
                if k in d and d[k] is not None:
                    return d[k]
            return default

        order_sn = safe_get(order, "order_sn", "sn", default="æœªçŸ¥")
        status = safe_get(order, "status", default="æœªçŸ¥")
        amount = safe_get(order, "total_amount", "amount", default=0)
        tracking = safe_get(order, "tracking_number", "tracking", "shipping_address", default=None)
        items = safe_get(order, "items", default=[])

        order_str = (
            f"ã€è®¢å•è¯¦æƒ…ã€‘:\n"
            f"- è®¢å•å·: {order_sn}\n"
            f"- å½“å‰çŠ¶æ€: {status}\n"
            f"- è®¢å•é‡‘é¢: {amount} å…ƒ\n"
            f"- æ”¶è´§åœ°å€: {tracking or 'æš‚æ— '}\n"
            f"- å•†å“æ˜ç»†: {items}"
        )
        context_parts.append(order_str)

    context_info = "\n\n".join(context_parts) if context_parts else "æš‚æ— ç›¸å…³å‚è€ƒä¿¡æ¯ã€‚"

    # 2. æ„å»ºå›ç­”
    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨äº† historyï¼Œè®© LLM å…·å¤‡å¤šè½®æ„ŸçŸ¥
    messages = [
        SystemMessage(content=GENERATE_PROMPT_TEMPLATE.format(
            context_info=context_info,
            question=state["question"]
        ))
    ]
    
    # å¯ä»¥åœ¨è¿™é‡ŒåŠ å…¥ state["history"] çš„æœ«å°¾å‡ æ¡ï¼Œå®ç°å¤šè½®å¯¹è¯
    
    response = await llm.ainvoke(messages)
    
    return {"answer": response.content}



# æ„å›¾è¯†åˆ«çš„ System Prompt
INTENT_PROMPT = """ä½ æ˜¯ä¸€ä¸ªç”µå•†å®¢æœåˆ†ç±»å™¨ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç”¨æˆ·çš„è¾“å…¥ï¼Œå°†å…¶å½’ç±»ä¸ºä»¥ä¸‹ä¸‰ç§æ„å›¾ä¹‹ä¸€ï¼š
- "ORDER": ç”¨æˆ·è¯¢é—®å…³äºä»–ä»¬è‡ªå·±çš„è®¢å•çŠ¶æ€ã€ç‰©æµã€è¯¦æƒ…ç­‰ã€‚
- "POLICY": ç”¨æˆ·è¯¢é—®å…³äºå¹³å°é€šç”¨çš„é€€æ¢è´§ã€è¿è´¹ã€æ—¶æ•ˆç­‰æ”¿ç­–ä¿¡æ¯ã€‚
- "OTHER": ç”¨æˆ·è¿›è¡Œé—²èŠã€æ‰“æ‹›å‘¼æˆ–æå‡ºä¸ä¸Šè¿°æ— å…³çš„é—®é¢˜ã€‚

åªè¿”å›åˆ†ç±»æ ‡ç­¾ï¼ˆORDER/POLICY/OTHERï¼‰ï¼Œä¸è¦è¿”å›ä»»ä½•å…¶ä»–æ–‡å­—ã€‚"""

async def intent_router(state: AgentState):
    """
    æ„å›¾è¯†åˆ«èŠ‚ç‚¹ï¼šåˆ¤æ–­ç”¨æˆ·æƒ³å¹²ä»€ä¹ˆ
    """
    print(f"ğŸ§  [Router] æ­£åœ¨åˆ†ææ„å›¾: {state['question']}")
    
    response = await llm.ainvoke([
        SystemMessage(content=INTENT_PROMPT),
        HumanMessage(content=state["question"])
    ])
    
    intent = response.content.strip().upper()
    # å®¹é”™å¤„ç†
    if intent not in ["ORDER", "POLICY", "OTHER"]:
        intent = "OTHER"
        
    print(f"ğŸ¯ [Router] è¯†åˆ«ç»“æœ: {intent}")
    return {"intent": intent}

async def query_order(state: AgentState):
    """
    è®¢å•æŸ¥è¯¢èŠ‚ç‚¹ï¼šä¸ä»…æŸ¥è¯¢æ•°æ®ï¼Œè¿˜è´Ÿè´£å°†æ•°æ®â€œè¯­è¨€åŒ–â€ä¾›ç”ŸæˆèŠ‚ç‚¹ä½¿ç”¨
    """
    question = state["question"]
    user_id = state["user_id"]
    
    import re
    order_sn_match = re.search(r'SN\d+', question.upper())
    
    # æ„é€ æŸ¥è¯¢
    if not order_sn_match:
        print("ğŸ” [QueryOrder] è·å–ç”¨æˆ·æœ€è¿‘è®¢å•")
        stmt = (
            select(Order)
            .where(Order.user_id == user_id)
            .order_by(Order.created_at.desc())
            .limit(1)
        )
    else:
        order_sn = order_sn_match.group()
        print(f"ğŸ” [QueryOrder] æŸ¥è¯¢è®¢å•å·: {order_sn}")
        stmt = select(Order).where(
            Order.order_sn == order_sn,
            Order.user_id == user_id 
        )

    async with async_session_maker() as session:
        # ä¿®æ­£ç‚¹ï¼šä½¿ç”¨ session.exec æ›´åŠ ç®€æ´ä¸”ç¬¦åˆ SQLModel å¼‚æ­¥è§„èŒƒ
        result = await session.exec(stmt)
        order = result.first()

    if not order:
        return {
            "order_data": None, 
            "context": ["ç”¨æˆ·è¯¢é—®äº†è®¢å•ï¼Œä½†æ•°æ®åº“ä¸­æœªæŸ¥åˆ°ç›¸å…³è®°å½•ã€‚"]
        }
    
    # è¿™æ · generate èŠ‚ç‚¹å°±å¯ä»¥åƒå¤„ç† RAG æ•°æ®ä¸€æ ·å¤„ç†è®¢å•æ•°æ®
    items_str = ", ".join([f"{i['name']}(x{i['qty']})" for i in order.items])
    order_context = (
        f"è®¢å•å·: {order.order_sn}\n"
        f"çŠ¶æ€: {order.status}\n"
        f"å•†å“: {items_str}\n"
        f"é‡‘é¢: {order.total_amount}å…ƒ\n"
        f"ç‰©æµå•å·: {order.tracking_number or 'æš‚æ— '}"
    )
    
    return {
        "order_data": order.model_dump(), 
        "context": [order_context] # å°†è®¢å•ä¿¡æ¯æ”¾å…¥ contextï¼Œç»Ÿä¸€äº¤ç»™ generate å¤„ç†
    }

